y = "Reviews found funny"
)
#400 hours
ggplot(df.original, aes(x=total_game_hours, fill=num_found_funny)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(300,400) + ylim(0,10) +
labs(
title = "Distribution of game hours and helpful reviews (100 hours)",
x = "Total game hours",
y = "Reviews found funny"
)
#500 hours
ggplot(df.original, aes(x=total_game_hours, fill=num_found_funny)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(400,500) + ylim(0,10) +
labs(
title = "Distribution of game hours and helpful reviews (100 hours)",
x = "Total game hours",
y = "Reviews found funny"
)
#100 hours
ggplot(df.original, aes(x=total_game_hours, fill=num_found_unhelpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(0,100) + ylim(0,10) +
labs(
title = "Distribution of game hours and helpful reviews (100 hours)",
x = "Total game hours",
y = "Reviews found unhelpful"
)
#200 hours
ggplot(df.original, aes(x=total_game_hours, fill=num_found_unhelpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(100,200) + ylim(0,10) +
labs(
title = "Distribution of game hours and helpful reviews (100 hours)",
x = "Total game hours",
y = "Reviews found unhelpful"
)
#300 hours
ggplot(df.original, aes(x=total_game_hours, fill=num_found_unhelpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(200,300) + ylim(0,10) +
labs(
title = "Distribution of game hours and helpful reviews (100 hours)",
x = "Total game hours",
y = "Reviews found unhelpful"
)
#400 hours
ggplot(df.original, aes(x=total_game_hours, fill=num_found_unhelpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(300,400) + ylim(0,10) +
labs(
title = "Distribution of game hours and helpful reviews (100 hours)",
x = "Total game hours",
y = "Reviews found unhelpful"
)
#500 hours
ggplot(df.original, aes(x=total_game_hours, fill=num_found_unhelpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(400,500) + ylim(0,10) +
labs(
title = "Distribution of game hours and helpful reviews (100 hours)",
x = "Total game hours",
y = "Reviews found unhelpful"
)
#print(df.original$review)
# Commented out for knitting purposes
df.original$is_short_review <- ifelse(nchar(df.original$review)<120,1,0)
#Short reviews found helpful or unhelpful
barplot(with(df.original, table(num_found_helpful, is_short_review)), main = "Short Reviews found Helpful or Unhelpful", col=1)
#Short reviews found funny
barplot(with(df.original, table(num_found_funny, is_short_review)), main = "Short Reviews found Funny or Unfunny", col=1)
#Set review is funny boolean
df.original$reviewIsFunny <- ifelse(df.original$num_found_funny>0,1,0)
table(df.original$reviewIsFunny)
ggplot(df.original, aes(x=reviewIsFunny, fill=num_found_helpful)) +
geom_bar(position=position_dodge()) +
coord_flip() +
labs(
title = "Relationship with Helpful/Unhelpful Reviews being found funny",
x = "Review is Funny",
y = "Num Helpful",
fill = "Region"
)
# Text Mining set was pulled from "https://www.hackerearth.com/practice/machine-learning/advanced-techniques/text-mining-feature-engineering-r/tutorial/"
library(data.table)
library(tm)
tdata <- df.original
tdata$review <- lapply(tdata$review, tolower)
text_corpus <- Corpus(VectorSource(tdata$review))
# Clean Corpus Data
dropword <- "br"
text_corpus <- tm_map(text_corpus, content_transformer(removeWords),dropword)
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
text_corpus <- tm_map(text_corpus, content_transformer(removePunctuation))
text_corpus <- tm_map(text_corpus, content_transformer(removeNumbers))
text_corpus <- tm_map(text_corpus, content_transformer(removeWords), c(stopwords('english')))
text_corpus <- tm_map(text_corpus, content_transformer(stripWhitespace))
# Stem the document so all plural words are now counted as one word
text_corpus <- tm_map(text_corpus, stemDocument, language = "english")
#print(as.character(text_corpus[[1]]))
#text_corpus[[1]]$content
docterm_corpus <- DocumentTermMatrix(text_corpus)
dim(docterm_corpus)
new_docterm_corpus <- removeSparseTerms(docterm_corpus,sparse = 0.95)
dim(new_docterm_corpus)
#find frequent terms
colS <- colSums(as.matrix(new_docterm_corpus))
length(colS)
doc_features <- data.table(name = attributes(colS)$names, count = colS)
#most frequent and least frequent words
doc_features[order(-count)][1:10] #top 10 most frequent words
doc_features[order(count)][1:10] #least 10 frequent words
processed_data <- as.data.table(as.matrix(new_docterm_corpus))
text_dataset <- cbind(df.original, processed_data)
#head(text_dataset)
#Commented out for knitting purposes
library("qdap")
install.packages("qdap")
library("qdap")
install.packages(c("broom", "cli", "dbplyr", "pkgload", "processx", "reprex"))
install.packages(c("broom", "cli", "dbplyr", "pkgload", "processx", "reprex"))
install.packages(c("broom", "cli", "dbplyr", "pkgload", "processx", "reprex"))
?droplevels
install.packages("Quandl")
install.packages(c("esquisse", "pillar", "RcppArmadillo", "RSQLite", "viridisLite"))
library(quandl)
library(Quandl)
library(pandas)
install.packages("pandas")
install.packages("PANDA")
install.packages("matlib")
install.packages("datetime")
install.packages("RcppArmadillo")
?PANDA
remove.packages("PANDA")
?Quandl
?Quandl.api_key
quandl.ApiConfig.api_key = 'xpLpDBByVG29tYUiuq27'
df = quandl.get("WIKI/AAPL")
#df = quandl.get("WIKI/AAPL")
Quandl.search("WIKI/AAPL")
#df = quandl.get("WIKI/AAPL")
quandl.api("WIKI/AAPL")
df = df[["Adj. Close"]]
#df = quandl.get("WIKI/AAPL")
quandl.api("WIKI/AAPL")
library(Quandl)
library(datetime)
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.linear_model import LinearRegression
quandl.ApiConfig.api_key = 'xpLpDBByVG29tYUiuq27'
#df = quandl.get("WIKI/AAPL")
quandl.api("WIKI/AAPL")
df = df[["Adj. Close"]]
df.head()
library(datetime)
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.linear_model import LinearRegression
quandl.ApiConfig.api_key = 'xpLpDBByVG29tYUiuq27'
#df = quandl.get("WIKI/AAPL")
df = quandl.api("WIKI/AAPL")
df = df[["Adj. Close"]]
head(df)
df
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
# Load airquality data
data("airquality")
str(airquality)
# Find missing values
is.na(airquality)
# Count missing values
sum(is.na(airquality))
# Or use summary() function
summary(airquality)
# Locate the positions with missing values in selected columns
which(is.na(airquality$Ozone))
which(is.na(airquality$Solar.R))
# Or you can use apply() function to identify all the columns with missing values
list_na <- colnames(airquality)[apply(airquality, 2, anyNA)]
list_na
# Or use summary() function
summary(airquality)
# Locate the positions with missing values in selected columns
which(is.na(airquality$Ozone))
which(is.na(airquality$Solar.R))
# Or you can use apply() function to identify all the columns with missing values
list_na <- colnames(airquality)[apply(airquality, 2, anyNA)]
list_na
# Visualize all the missing values
library(naniar)
vis_miss(airquality)
# Drop rows with missing values
na.omit(airquality)
# If dropping all missing values impact your data size dramatically, you may replace those NAs with other values
airquality$Ozone[is.na(airquality$Ozone)] <- mean(airquality$Ozone, na.rm=TRUE)
airquality$Ozone
#Loading the mice package
library(mice)
data(nhanes)
str(nhanes)
head(nhanes)
library(lattice)
#Convert Age to factor
nhanes$age=as.factor(nhanes$age)
#understand the missing value pattern
md.pattern(nhanes)
library(VIM)
#plot the missing values
nhanes_miss = aggr(nhanes, col=mdc(1:2), numbers=TRUE,
sortVars=TRUE, labels=names(nhanes),
cex.axis=.7, gap=3,
ylab=c("Proportion of missingness","Missingness Pattern"))
library(VIM)
#plot the missing values
nhanes_miss = aggr(nhanes, col=mdc(1:2), numbers=TRUE,
sortVars=TRUE, labels=names(nhanes),
cex.axis=.7, gap=3,
ylab=c("Proportion of missingness","Missingness Pattern"))
library(VIM)
#plot the missing values
nhanes_miss = aggr(nhanes, col=mdc(1:2), numbers=TRUE,
sortVars=TRUE, labels=names(nhanes),
cex.axis=.7, gap=3,
ylab=c("Proportion of missingness","Missingness Pattern"))
library(VIM)
#plot the missing values
nhanes_miss = aggr(nhanes, col=mdc(1:2), numbers=TRUE,
sortVars=TRUE, labels=names(nhanes),
cex.axis=.7, gap=3,
ylab=c("Proportion of missingness","Missingness Pattern"))
#Imputing missing values using mice
mice_imputes <- mice(nhanes, m=5, maxit = 40)
#Imputing missing values using mice
mice_imputes <- mice(nhanes, m=5, maxit = 40)
#Imputing missing values using mice
mice_imputes <- mice(nhanes, m=8, maxit = 40)
#Imputing missing values using mice
mice_imputes <- mice(nhanes, m=3, maxit = 40)
#Imputing missing values using mice
mice_imputes <- mice(nhanes, m=5, maxit = 40)
#Imputing missing values using mice
mice_imputes <- mice(nhanes, m=1, maxit = 40)
#Imputing missing values using mice
mice_imputes <- mice(nhanes, m=100, maxit = 40)
#Imputing missing values using mice
mice_imputes <- mice(nhanes, m=5, maxit = 40)
# Check methods were used for imputing
mice_imputes$method
# Let's look at our imputed values for chl
mice_imputes$imp$chl
# Plotting and comparing values with xyplot()
xyplot(mice_imputes, bmi ~ chl | .imp, pch = 20, cex = 1.4)
?imputed
?impute
# Imputed dataset
# Since the 4th dataset gave us the best fitting, I will impute the missing values from it.
Imputed_data=complete(mice_imputes,5)
summary(Imputed_data)
knitr::opts_chunk$set(echo = TRUE)
# Clean the environment
rm(list = ls())
set.seed(1)
x <- rnorm(100)
# Intentionally add 3 outliers
x <- c(x,-3.5, 3.3, 4.1)
set.seed(1)
x <- rnorm(100)
c
v
x
x <- rnorm(100)
x <- rnorm(100)
x <- rnorm(100)
set.seed(1)
x <- rnorm(100)
set.seed(1)
x <- rnorm(100)
x
# Intentionally add 3 outliers
x <- c(x,-3.5, 3.3, 4.1)
x
plot(x)
# Create boxplot
bp <- boxplot(x)
# Print boxplot object
print(bp)
# Print outliers
bp$out
# Generate 100 random values
set.seed(1)
x <- rnorm(100)
mean(x)
sd(x)
# Intentionally add 3 potential outliers
x <- c(x,-3.5, 3.3, 4.1)
# Calculate mean
mean(x)
# Calculate standard deviation
sd(x)
# Calculate z-score
z <- (x - mean(x))/sd(x)
# Print outliers
x[which(z > 3 | z < -3)]
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(dplyr)
library(BatchGetSymbols)
library(caret)
library(ggplot2)
library(scales)
library(date)
library(DAAG)
library(mlbench)
library(tidyverse)
library(leaps)
library(glmnet)
library(GGally)
library(esquisse)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(dplyr)
library(BatchGetSymbols)
library(caret)
library(ggplot2)
library(scales)
library(date)
library(DAAG)
library(mlbench)
library(tidyverse)
library(leaps)
library(glmnet)
library(GGally)
library(esquisse)
# set dates
first.date <- Sys.Date() - 120
last.date <- Sys.Date()
freq.data <- 'daily'
# set tickers
tickers <- c('aapl')
df <- as.data.frame(BatchGetSymbols(tickers = tickers,
first.date = first.date,
last.date = last.date,
freq.data = freq.data,
cache.folder = file.path(tempdir(),
'BGS_Cache') ), stringsAsFactors = TRUE) # cache in tempdir()
# remove variables
df <- subset(df, select = -c(df.control.src, df.control.download.status, df.control.perc.benchmark.dates, df.control.threshold.decision, df.control.total.obs, df.control.ticker))
df <- subset(df,select = -c(df.tickers.ret.adjusted.prices, df.tickers.ret.closing.prices))
df$df.tickers.ref.date <- as.POSIXct(df$df.tickers.ref.date, format = "%Y-%m-%d")
#df$df.tickers.ref.date <- format.Date(df$df.tickers.ref.date)
#df$df.tickers.ref.date <- c(1:91)
fitControl <- trainControl(method = "cv", number = 5)
set.seed(123)
scatter.smooth(x=df$df.tickers.ref.date, y=df$df.tickers.price.close, main= tickers)  # scatterplot
esquisser(data = df)
esquisser(data = df)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(dplyr)
library(BatchGetSymbols)
library(caret)
library(ggplot2)
library(scales)
library(date)
library(DAAG)
# set dates
first.date <- Sys.Date() - 1
last.date <- Sys.Date()
freq.data <- 'daily'
# set tickers
tickers <- c('doge-usd')
df <- as.data.frame(BatchGetSymbols(tickers = tickers,
first.date = first.date,
last.date = last.date,
freq.data = freq.data,
cache.folder = file.path(tempdir(),
'BGS_Cache') ), stringsAsFactors = TRUE) # cache in tempdir()
df <- subset(df, select = -c(df.control.src, df.control.download.status, df.control.perc.benchmark.dates, df.control.threshold.decision, df.control.total.obs, df.control.ticker))
df <- subset(df,select = -c(df.tickers.ret.adjusted.prices, df.tickers.ret.closing.prices))
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(dplyr)
library(BatchGetSymbols)
library(caret)
library(ggplot2)
library(scales)
library(date)
library(DAAG)
# set dates
first.date <- Sys.Date() - 1
last.date <- Sys.Date()
freq.data <- 'daily'
# set tickers
tickers <- c('doge-usd')
df <- as.data.frame(BatchGetSymbols(tickers = tickers,
first.date = first.date,
last.date = last.date,
freq.data = freq.data,
cache.folder = file.path(tempdir(),
'BGS_Cache') ), stringsAsFactors = TRUE) # cache in tempdir()
df <- subset(df, select = -c(df.control.src, df.control.download.status, df.control.perc.benchmark.dates, df.control.threshold.decision, df.control.total.obs, df.control.ticker))
df <- subset(df,select = -c(df.tickers.ret.adjusted.prices, df.tickers.ret.closing.prices))
df$df.tickers.ref.date <- format.Date(df$df.tickers.ref.date)
df <- na.omit(df)
df$df.tickers.ref.date <- c(1:11)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(dplyr)
library(BatchGetSymbols)
library(caret)
library(ggplot2)
library(scales)
library(date)
library(DAAG)
# set dates
first.date <- Sys.Date() - 1
last.date <- Sys.Date()
freq.data <- 'daily'
# set tickers
tickers <- c('doge-usd')
df <- as.data.frame(BatchGetSymbols(tickers = tickers,
first.date = first.date,
last.date = last.date,
freq.data = freq.data,
cache.folder = file.path(tempdir(),
'BGS_Cache') ), stringsAsFactors = TRUE) # cache in tempdir()
df <- subset(df, select = -c(df.control.src, df.control.download.status, df.control.perc.benchmark.dates, df.control.threshold.decision, df.control.total.obs, df.control.ticker))
df <- subset(df,select = -c(df.tickers.ret.adjusted.prices, df.tickers.ret.closing.prices))
df$df.tickers.ref.date <- format.Date(df$df.tickers.ref.date)
df <- na.omit(df)
df$df.tickers.ref.date <- c(1:2)
fitControl <- trainControl(method = "cv", number = 5)
set.seed(123)
scatter.smooth(x=df$df.tickers.ref.date, y=df$df.tickers.price.close, main= tickers)  # scatterplot
lm <- lm(df.tickers.ref.date ~ df.tickers.price.close, data = df)
lm
summary(lm)
plot(lm)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(dplyr)
library(BatchGetSymbols)
library(caret)
library(ggplot2)
library(scales)
library(date)
library(DAAG)
# set dates
first.date <- Sys.Date() - 2
last.date <- Sys.Date()
freq.data <- 'daily'
# set tickers
tickers <- c('doge-usd')
df <- as.data.frame(BatchGetSymbols(tickers = tickers,
first.date = first.date,
last.date = last.date,
freq.data = freq.data,
cache.folder = file.path(tempdir(),
'BGS_Cache') ), stringsAsFactors = TRUE) # cache in tempdir()
df <- subset(df, select = -c(df.control.src, df.control.download.status, df.control.perc.benchmark.dates, df.control.threshold.decision, df.control.total.obs, df.control.ticker))
df <- subset(df,select = -c(df.tickers.ret.adjusted.prices, df.tickers.ret.closing.prices))
df$df.tickers.ref.date <- format.Date(df$df.tickers.ref.date)
df <- na.omit(df)
df$df.tickers.ref.date <- c(1:3)
fitControl <- trainControl(method = "cv", number = 5)
set.seed(123)
scatter.smooth(x=df$df.tickers.ref.date, y=df$df.tickers.price.close, main= tickers)  # scatterplot
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(dplyr)
library(BatchGetSymbols)
library(caret)
library(ggplot2)
library(scales)
library(date)
library(DAAG)
# set dates
first.date <- Sys.Date() - 2
last.date <- Sys.Date()
freq.data <- 'daily'
# set tickers
tickers <- c('doge-usd')
df <- as.data.frame(BatchGetSymbols(tickers = tickers,
first.date = first.date,
last.date = last.date,
freq.data = freq.data,
cache.folder = file.path(tempdir(),
'BGS_Cache') ), stringsAsFactors = TRUE) # cache in tempdir()
df <- subset(df, select = -c(df.control.src, df.control.download.status, df.control.perc.benchmark.dates, df.control.threshold.decision, df.control.total.obs, df.control.ticker))
df <- subset(df,select = -c(df.tickers.ret.adjusted.prices, df.tickers.ret.closing.prices))
df$df.tickers.ref.date <- format.Date(df$df.tickers.ref.date)
df <- na.omit(df)
df$df.tickers.ref.date <- c(1:2)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(dplyr)
library(BatchGetSymbols)
library(caret)
library(ggplot2)
library(scales)
library(date)
library(DAAG)
# set dates
first.date <- Sys.Date() - 2
last.date <- Sys.Date()
freq.data <- 'daily'
# set tickers
tickers <- c('doge-usd')
df <- as.data.frame(BatchGetSymbols(tickers = tickers,
first.date = first.date,
last.date = last.date,
freq.data = freq.data,
cache.folder = file.path(tempdir(),
'BGS_Cache') ), stringsAsFactors = TRUE) # cache in tempdir()
df <- subset(df, select = -c(df.control.src, df.control.download.status, df.control.perc.benchmark.dates, df.control.threshold.decision, df.control.total.obs, df.control.ticker))
df <- subset(df,select = -c(df.tickers.ret.adjusted.prices, df.tickers.ret.closing.prices))
df$df.tickers.ref.date <- format.Date(df$df.tickers.ref.date)
df <- na.omit(df)
df$df.tickers.ref.date <- c(1:3)
fitControl <- trainControl(method = "cv", number = 5)
set.seed(123)
scatter.smooth(x=df$df.tickers.ref.date, y=df$df.tickers.price.close, main= tickers)  # scatterplot
